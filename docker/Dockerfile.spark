# Bitnami 이미지는 Java, Python, Spark가 이미 잘 버무려져 있습니다.
FROM bitnami/spark:3.5.1

USER root

# 파이썬 잡/라이브러리 필요하면 설치
RUN pip install --no-cache-dir pandas pyarrow boto3

# 1. S3(MinIO) 연동을 위한 JAR 다운로드용 curl 설치
RUN install_packages curl

# 2. S3A 커넥터 JAR 설치 (Spark 3.5.1 / Hadoop 3.3.4 대응)
# 2차 때 S3 연동 안 해보셨으면 이 부분이 이번 프로젝트의 핵심입니다.
RUN curl -O https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    curl -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar && \
    mv *.jar /opt/bitnami/spark/jars/

# 3. Airflow와 통신하거나 ML 전처리에 필요한 라이브러리 설치
# 기존에 만든 requirements.txt를 활용합니다.
COPY requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# 다시 비루트 사용자로 전환 (보안 및 Bitnami 표준)
USER 1001
WORKDIR /opt/spark/jobs